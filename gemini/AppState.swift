import Foundation
import SwiftUI

@MainActor
class AppState: ObservableObject, SpeechRecognitionDelegate {
    
    // MARK: - Navigation States (ìƒˆë¡œìš´ 3ë‹¨ê³„ ì‹œìŠ¤í…œ)
    enum NavigationStage: String, CaseIterable {
        case sttScanningMode = "STT_Scanning_Mode"           // Stage 1: STT + AR ìŠ¤ìº” (Live API ì—°ê²° ì—†ìŒ)
        case liveGuidanceMode = "Live_Guidance_Mode"         // Stage 2: Live API + ì£¼ê¸°ì  ê°€ì´ë˜ìŠ¤
        case pureConversationMode = "Pure_Conversation_Mode" // Stage 3: ìˆœìˆ˜ ëŒ€í™”
    }
    
    // MARK: - Published Properties
    @Published var currentStage: NavigationStage = .sttScanningMode
    @Published var requestedObjectNameByUser: String = ""
    @Published var confirmedDetrObjectName: String? = nil
    @Published var isNavigationActive: Bool = false
    @Published var lastDetectedSpeech: String = "" // For debugging STT
    
    // MARK: - Stage 1 Specific Properties
    @Published var scannedObjectLabels: Set<String> = [] // 360ë„ ìŠ¤ìº” ì¤‘ ëˆ„ì ëœ ê°ì²´ë“¤
    @Published var scanProgress: Float = 0.0 // 360ë„ íšŒì „ ì§„í–‰ë¥  (0.0 ~ 1.0)
    @Published var isFullRotationComplete: Bool = false
    @Published var scanCompleted: Bool = false // âœ… ì¶”ê°€: ìŠ¤ìº” ì™„ë£Œ í”Œë˜ê·¸ (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
    @Published var objectMatchingInProgress: Bool = false // âœ… ì¶”ê°€: ê°ì²´ ë§¤ì¹­ ì§„í–‰ í”Œë˜ê·¸ (ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€)
    
    // **Stage 1â†’2 ì „í™˜ì„ ìœ„í•œ ì¤‘ì‹¬ë„ ê¸°ì¤€**
    private let requiredCenteredness: CGFloat = 0.85 // âœ… 85% ì¤‘ì‹¬ë„ + 1ì´ˆ ìœ ì§€ í•„ìš”
    
    // MARK: - Stage 2 Specific Properties  
    @Published var guidanceTimer: Timer? = nil // 3ì´ˆë§ˆë‹¤ ìë™ í”„ë¡¬í”„íŠ¸ìš© íƒ€ì´ë¨¸
    @Published var lastGuidanceTime: Date = Date.distantPast
    @Published var guidanceRequestCount: Int = 0 // ê°€ì´ë˜ìŠ¤ ìš”ì²­ íšŸìˆ˜
    
    // âœ… Stage 2â†’3 ì „í™˜ ì•ˆì •ì„±ì„ ìœ„í•œ í”„ë¡œí¼í‹°
    @Published var stage3TransitionStartTime: Date? = nil
    private let stage3TransitionRequiredDuration: TimeInterval = 2.0 // 2ì´ˆê°„ ì¡°ê±´ ìœ ì§€ í•„ìš”
    
    // âœ… ì¤‘ë³µ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ í”„ë¡œí¼í‹°
    @Published var isGuidanceRequestInProgress: Bool = false
    
    // MARK: - Audio Integration
    @Published var audioManager: AudioManager? = nil
    
    // MARK: - STT Integration
    @Published var speechManager: SpeechRecognitionManager
    
    // MARK: - AR Integration
    weak var arViewModel: ARViewModel?
    
    // MARK: - Gemini Integration  
    weak var geminiClient: GeminiLiveAPIClient?
    
    init() {
        self.speechManager = SpeechRecognitionManager()
        // AudioManagerëŠ” ë‚˜ì¤‘ì— setupAudioManager()ì—ì„œ ì´ˆê¸°í™”
        Task { @MainActor in
            self.speechManager.delegate = self
            // AudioManager ì´ˆê¸°í™”ëŠ” Gemini Live API ì—°ê²° í›„ë¡œ ì—°ê¸°
        }
    }
    
    // âœ… AudioManager ëŠ¦ì€ ì´ˆê¸°í™” ë©”ì„œë“œ
    private func setupAudioManager() {
        guard audioManager == nil else { return }
        
        print("AppState: Setting up AudioManager after Gemini Live API initialization")
        audioManager = AudioManager()
        audioManager?.checkAvailableAudioFiles()
        print("AppState: âœ… AudioManager setup completed")
    }
    
    // MARK: - STT Control
    func startListeningForKeywords() {
        guard currentStage == .sttScanningMode else {
            print("AppState: Not starting STT - not in STT scanning stage")
            return
        }
        
        // **Stage 1ì—ì„œëŠ” Gemini ì†Œì¼“ ì—°ê²°í•˜ì§€ ì•ŠìŒ**
        if let geminiClient = geminiClient, geminiClient.isConnected {
            print("AppState: Disconnecting Gemini socket for Stage 1 (STT only)")
            geminiClient.disconnect()
        }
        
        // STT ì‹œì‘
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            self.speechManager.startListening()
            print("AppState: Started listening for Korean keywords in Stage 1")
        }
    }
    
    func stopListeningForKeywords() {
        speechManager.stopListening()
        print("AppState: Stopped listening for keywords")
    }
    
    // MARK: - SpeechRecognitionDelegate
    nonisolated func didDetectFindRequest(for objectName: String, in fullText: String) {
        Task { @MainActor in
            // Stage 1ì—ì„œë§Œ ê°ì²´ ì°¾ê¸° ìš”ì²­ ì²˜ë¦¬
            guard currentStage == .sttScanningMode else {
                print("AppState: Ignoring find request - not in STT scanning stage")
                return
            }
            
            lastDetectedSpeech = fullText
            setRequestedObject(objectName)
            
            // STT ì¤‘ì§€
            speechManager.stopListening()
            print("AppState: STT stopped after keyword detection in Stage 1")
            
            // âœ… 5ì´ˆ ìŠ¤ìº”ì´ ì™„ë£Œë˜ì—ˆë‹¤ë©´ ì¦‰ì‹œ ê°ì²´ ë§¤ì¹­ ì§„í–‰
            if scanCompleted && !scannedObjectLabels.isEmpty {
                print("AppState: 5-second scan already completed, proceeding with object matching")
                requestGeminiObjectMatching()
            } else {
                print("AppState: 5-second scan not completed yet, starting AR scanning")
                // AR ìŠ¤ìº” ì‹œì‘
                startARScanning()
            }
        }
    }
    
    // MARK: - Stage Management
    func transitionTo(_ newStage: NavigationStage) {
        let previousStage = currentStage
        currentStage = newStage
        
        // âœ… Stage ì „í™˜ ì‹œ íƒ€ì´ë¨¸ ë¦¬ì…‹
        stage3TransitionStartTime = nil
        
        print("AppState: Transitioned from \(previousStage.rawValue) to \(newStage.rawValue)")
        
        // âœ… ë‹¨ìˆœí™”: Stage ì „í™˜ ì‹œ ê³¼ë„í•œ cleanup ì œê±°
        
        isNavigationActive = (newStage == .liveGuidanceMode || newStage == .pureConversationMode)
        
        handleStageEntry(newStage, from: previousStage)
    }
    
    private func handleStageEntry(_ stage: NavigationStage, from previousStage: NavigationStage) {
        switch stage {
        case .sttScanningMode:
            print("AppState: === STAGE 1: STT + AR Scanning Mode ===")
            
            // Reset all data
            requestedObjectNameByUser = ""
            confirmedDetrObjectName = nil
            isNavigationActive = false
            lastDetectedSpeech = ""
            scannedObjectLabels = []
            scanProgress = 0.0
            isFullRotationComplete = false
            scanCompleted = false // âœ… ì¶”ê°€: ìŠ¤ìº” ì™„ë£Œ í”Œë˜ê·¸ ë¦¬ì…‹
            objectMatchingInProgress = false // âœ… ì¶”ê°€: ê°ì²´ ë§¤ì¹­ í”Œë˜ê·¸ ë¦¬ì…‹
            guidanceRequestCount = 0
            
            stopPeriodicGuidance()
            
            // Disconnect Gemini for Stage 1 (STT only)
            if let geminiClient = geminiClient, geminiClient.isConnected {
                print("AppState: Disconnecting Gemini for Stage 1 (STT only)")
                geminiClient.disconnect()
            }
            
            arViewModel?.stopScanning()
            arViewModel?.stopHapticGuidance()
            
            // âœ… Audio-driven workflow: í™˜ì˜ ë©”ì‹œì§€ í›„ STT ì‹œì‘
            if audioManager == nil {
                print("AppState: AudioManager not ready, setting up temporarily for Stage 1")
                setupAudioManager()
            }
            
            // âœ… ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°: 5ì´ˆ ìŠ¤ìº” â†’ ìë™ ì§ˆë¬¸
            audioManager?.playWelcomeRotateAudio { [weak self] in
                print("AppState: Welcome audio completed, starting 5-second scan")
                self?.startTenSecondScan()
            }
            
        case .liveGuidanceMode:
            print("AppState: === STAGE 2: Live Guidance Mode ===")
            
            speechManager.stopListening()
            print("AppState: STT stopped before Gemini connection")
            
            // âœ… KEEP HAPTIC: í–…í‹± ê°€ì´ë˜ìŠ¤ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
            // arViewModel?.stopHapticGuidance()  // âŒ ì£¼ì„ ì²˜ë¦¬
            
            // âœ… Audio confirmation: íƒ€ê²Ÿ ë½ì˜¨ ì•ˆë‚´
            if let distance = arViewModel?.distanceToObject {
                audioManager?.playTargetLockedAudio(distance: distance) { [weak self] in
                    print("AppState: Target locked audio completed, starting guidance")
                    self?.connectToGeminiLiveAPI()
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                        self?.startPeriodicGuidance()
                    }
                }
            } else {
                // ê±°ë¦¬ ì •ë³´ ì—†ì„ ê²½ìš°ì—ë„ ì˜¤ë””ì˜¤ íŒŒì¼ ì‚¬ìš©
                audioManager?.playTargetLockedAudio(distance: 0.0) { [weak self] in
                    print("AppState: Target locked audio completed, starting guidance")
                    self?.connectToGeminiLiveAPI()
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                        self?.startPeriodicGuidance()
                    }
                }
            }
            
        case .pureConversationMode:
            print("AppState: === STAGE 3: Pure Conversation Mode ===")
            
            stopPeriodicGuidance()
            
            // âœ… STAGE 3: ë¬´ì¡°ê±´ í–…í‹± í”¼ë“œë°± ì¤‘ë‹¨ (Pure Conversation Mode)
            arViewModel?.stopHapticGuidance()
            print("AppState: Stopped haptic guidance - entering pure conversation mode")
            
            // âœ… ì¶”ê°€: íƒ€ê²Ÿ ì¶”ì  ì™„ì „ ì¤‘ë‹¨
            arViewModel?.userTargetObjectName = ""
            print("AppState: Cleared target object name - no more tracking")
            
            // âœ… ë‹¨ìˆœí™”: Stage 3 ì§„ì… ì‹œ ì¤‘ë³µ ì˜¤ë””ì˜¤ ì¤‘ë‹¨ ì œê±°
            // ë…¹ìŒì€ ê³„ì† ìœ ì§€í•˜ì—¬ ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•¨
            
            // âœ… AudioManagerì˜ í˜„ì¬ ì¬ìƒë„ ì¤‘ë‹¨
            audioManager?.stopAudio()
            
            // âœ… Stage 3: ì¬ì—°ê²° ì—†ì´ ë°”ë¡œ ëŒ€í™” ëª¨ë“œ í™œì„±í™” (ì´ë¯¸ Google Search í¬í•¨ëœ ì—°ê²° ì‚¬ìš©)
            if let geminiClient = geminiClient {
                print("ğŸ”Œ AppState: Activating Stage 3 conversation mode (no reconnection needed)")
                geminiClient.enablePureConversationMode()
            }
            
        }
    }
    
    // MARK: - Stage 1: AR Scanning Control
    
    // âœ… ìƒˆë¡œìš´ ë©”ì„œë“œ: 10ì´ˆê°„ ìë™ ìŠ¤ìº”
    private func startTenSecondScan() {
        guard currentStage == .sttScanningMode else {
            print("AppState: Not starting scan - not in STT scanning stage")
            return
        }
        
        print("AppState: ğŸ” Starting 5-second automatic scanning")
        print("AppState: â° Timer set for 5 seconds - will auto-ask for object")
        
        // AR ìŠ¤ìº” ì‹œì‘ (ê°ì²´ ì´ë¦„ ì—†ì´)
        arViewModel?.startScanning(for: "")
        
        // 5ì´ˆ ìŠ¤ìº” ì§„í–‰ ëª¨ë‹ˆí„°ë§
        monitorTenSecondScan()
        
        // 5ì´ˆ í›„ ìë™ìœ¼ë¡œ ì§ˆë¬¸ ì‹œì‘
        DispatchQueue.main.asyncAfter(deadline: .now() + 5.0) {
            self.completeTenSecondScanAndAskUser()
        }
    }
    
    private func monitorTenSecondScan() {
        guard currentStage == .sttScanningMode, !scanCompleted else { 
            return 
        }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            guard self.currentStage == .sttScanningMode, !self.scanCompleted,
                  let arViewModel = self.arViewModel else { 
                return 
            }
            
            // ìŠ¤ìº”ëœ ê°ì²´ë“¤ ëˆ„ì 
            self.scannedObjectLabels = arViewModel.allDetectedObjects
            
            // ê³„ì† ëª¨ë‹ˆí„°ë§ (10ì´ˆ ë™ì•ˆ)
            if !self.scanCompleted {
                self.monitorTenSecondScan()
            }
        }
    }
    
    private func completeTenSecondScanAndAskUser() {
        guard currentStage == .sttScanningMode, !scanCompleted else {
            print("AppState: âš ï¸ 10-second scan already completed or stage changed")
            return
        }
        
        scanCompleted = true // âœ… ìŠ¤ìº” ì™„ë£Œ í‘œì‹œ
        print("AppState: âœ… 5-second scan completed. Found \(scannedObjectLabels.count) objects: \(Array(scannedObjectLabels))")
        
        // AR ìŠ¤ìº” ì¤‘ì§€
        arViewModel?.stopScanning()
        
        // âœ… ì´ë¯¸ ìœ ì €ê°€ ê°ì²´ë¥¼ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
        if !requestedObjectNameByUser.isEmpty {
            print("AppState: User already requested object '\(requestedObjectNameByUser)', proceeding with matching")
            requestGeminiObjectMatching()
        } else {
            // ìë™ìœ¼ë¡œ ì§ˆë¬¸ ì˜¤ë””ì˜¤ ì¬ìƒ í›„ STT ì‹œì‘
            audioManager?.playAskObjectAudio { [weak self] in
                print("AppState: Ask what object audio completed, starting STT")
                self?.startListeningForKeywords()
            }
        }
    }
    
    private func startARScanning() {
        print("AppState: Starting AR scanning for: '\(requestedObjectNameByUser)'")
        
        // AR ìŠ¤ìº” ì‹œì‘
        arViewModel?.startScanning(for: requestedObjectNameByUser)
        
        // ìŠ¤ìº” ì§„í–‰ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitorScanProgress()
    }
    
    private func monitorScanProgress() {
        guard currentStage == .sttScanningMode, !scanCompleted else { 
            if scanCompleted {
                print("AppState: âš ï¸ Scan already completed, skipping monitorScanProgress")
            }
            return 
        }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            guard self.currentStage == .sttScanningMode, !self.scanCompleted,
                  let arViewModel = self.arViewModel else { 
                if self.scanCompleted {
                    print("AppState: âš ï¸ Scan completed during monitoring, stopping")
                }
                return 
            }
            
            // Sync detected objects from ARViewModel
            self.scannedObjectLabels = arViewModel.allDetectedObjects
            self.scanProgress = arViewModel.scanProgress
            
            if arViewModel.scanProgress >= 1.0 {
                print("AppState: ğŸ¯ Scan progress completed, calling completeScan()")
                self.completeScan()
            } else {
                self.monitorScanProgress()
            }
        }
    }
    
    private func completeScan() {
        // âœ… ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        guard !scanCompleted else {
            print("AppState: âš ï¸ completeScan() already called, ignoring duplicate call")
            return
        }
        
        guard let arViewModel = arViewModel else { return }
        
        scanCompleted = true // âœ… í”Œë˜ê·¸ ì„¤ì •ìœ¼ë¡œ ì¶”ê°€ í˜¸ì¶œ ë°©ì§€
        
        // Always sync detected objects before proceeding
        scannedObjectLabels = arViewModel.allDetectedObjects
        print("AppState: âœ… Scan completed. Found target: \(arViewModel.foundTarget)")
        print("AppState: ğŸ“‹ Total detected objects: \(scannedObjectLabels.count) - \(Array(scannedObjectLabels).sorted())")
        
        // Log scan completion summary
        
        // âœ… 5ì´ˆ ìë™ ìŠ¤ìº” ì¤‘ì´ì—ˆë‹¤ë©´ ë‹¨ìˆœíˆ ëˆ„ì ë§Œ í•˜ê³  completeTenSecondScanAndAskUserê°€ ì²˜ë¦¬
        if requestedObjectNameByUser.isEmpty {
            print("AppState: This was automatic 5-second scan, letting completeTenSecondScanAndAskUser handle it")
            return
        }
        
        // âœ… ì‚¬ìš©ìê°€ íŠ¹ì • ê°ì²´ë¥¼ ìš”ì²­í•œ ìƒíƒœì—ì„œì˜ ìŠ¤ìº” ì™„ë£Œ
        if arViewModel.foundTarget {
            confirmedDetrObjectName = requestedObjectNameByUser
            arViewModel.userTargetObjectName = requestedObjectNameByUser
            startHapticGuidanceAndTransition()
        } else {
            requestGeminiObjectMatching()
        }
    }
    
    private func requestGeminiObjectMatching() {
        // âœ… ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        guard !objectMatchingInProgress else {
            print("AppState: âš ï¸ Object matching already in progress, ignoring duplicate call")
            return
        }
        
        guard let geminiClient = geminiClient,
              let arViewModel = arViewModel else {
            print("AppState: Missing geminiClient or arViewModel for object matching")
            
            // âœ… Audio-driven transition: ê°ì²´ ì—†ìŒ ì•ˆë‚´
            audioManager?.playObjectNotFoundAudio { [weak self] in
                print("AppState: Object not found audio completed, transitioning to Stage 3")
                self?.transitionTo(.pureConversationMode)
            }
            return
        }
        
        // âœ… ìµœì¢… ë™ê¸°í™”: ê°ì²´ ë§¤ì¹­ ì§ì „ì— ìµœì‹  ìƒíƒœë¡œ ë™ê¸°í™”
        scannedObjectLabels = arViewModel.allDetectedObjects
        print("AppState: Final sync before matching - \(scannedObjectLabels.count) objects")
        
        let detectedObjects = Array(scannedObjectLabels).sorted() // Sort for consistent output
        guard !detectedObjects.isEmpty else {
            print("AppState: No objects detected for matching")
            // No objects detected
            
            // âœ… Audio-driven transition: ê°ì²´ ì—†ìŒ ì•ˆë‚´
            audioManager?.playObjectNotFoundAudio { [weak self] in
                print("AppState: Object not found audio completed, transitioning to Stage 3")
                self?.transitionTo(.pureConversationMode)
            }
            return
        }
        
        objectMatchingInProgress = true // âœ… í”Œë˜ê·¸ ì„¤ì •ìœ¼ë¡œ ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€
        print("AppState: ğŸ” Requesting Gemini API matching for '\(requestedObjectNameByUser)' among: \(detectedObjects)")
        // Start object matching
        
        // âœ… WebSocket ì—°ê²° ì œê±° - REST APIëŠ” WebSocket ì—†ì´ë„ ì‘ë™
        // if !geminiClient.isConnected {
        //     geminiClient.connect()
        // }
        
        geminiClient.findSimilarObject(koreanObjectName: requestedObjectNameByUser, availableObjects: detectedObjects) { [weak self] matchedObject in
            guard let self = self else { return }
            
            // âœ… í”Œë˜ê·¸ í•´ì œ
            DispatchQueue.main.async {
                self.objectMatchingInProgress = false
            }
            
            if let matchedObject = matchedObject {
                print("AppState: âœ… Gemini matched '\(self.requestedObjectNameByUser)' to '\(matchedObject)'")
                self.confirmedDetrObjectName = matchedObject
                
                DispatchQueue.main.async {
                    self.arViewModel?.userTargetObjectName = matchedObject
                    
                    // âœ… Audio + Haptic: ë™ì‹œ ì‹œì‘ (Critical Requirement)
                    self.audioManager?.playObjectFoundHapticGuideAudio { [weak self] in
                        print("AppState: Object found haptic guide audio completed")
                    }
                    
                    // âœ… ê°ì²´ ë°œê²¬ íŠ¹ë³„ í–…í‹± íŒ¨í„´ (ì‹¬ì¥ë°•ë™)
                    self.arViewModel?.playObjectFoundHaptic()
                    
                    // í–…í‹± ê°€ì´ë˜ìŠ¤ë„ ë™ì‹œì— ì‹œì‘
                    self.startHapticGuidanceAndTransition()
                }
            } else {
                print("AppState: âŒ Gemini could not find a match for '\(self.requestedObjectNameByUser)'")
                
                // âœ… Audio-driven transition: ê°ì²´ ì—†ìŒ ì•ˆë‚´
                self.audioManager?.playObjectNotFoundAudio { [weak self] in
                    print("AppState: Object not found audio completed, transitioning to Stage 3")
                    self?.transitionTo(.pureConversationMode)
                }
            }
        }
    }
    
    private func startHapticGuidanceAndTransition() {
        guard let confirmedObject = confirmedDetrObjectName else { return }
        
        print("AppState: Starting haptic guidance for: '\(confirmedObject)'")
        
        // í–…í‹± ê°€ì´ë“œ ì‹œì‘
        arViewModel?.startHapticGuidance(for: confirmedObject)
        
        // íƒ€ê²Ÿ ë„ë‹¬ ëª¨ë‹ˆí„°ë§
        monitorTargetReached()
    }
    
    private func monitorTargetReached() {
        guard currentStage == .sttScanningMode else { return }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            guard self.currentStage == .sttScanningMode,
                  let arViewModel = self.arViewModel else { return }
            
            // âœ… ARViewModelì˜ ì¤‘ì•™ íƒì§€ ì™„ë£Œ ìƒíƒœ í™•ì¸ (85%, 1ì´ˆ ìœ ì§€)
            let currentCenteredness = arViewModel.detectedObjectCenteredness
            let isCenterDetectionCompleted = arViewModel.isCenterDetectionActive
            
            if isCenterDetectionCompleted {
                // âœ… 85% ì¤‘ì‹¬ë„ë¥¼ 1ì´ˆê°„ ìœ ì§€ ì™„ë£Œ - Stage 2ë¡œ ì „í™˜
                print("AppState: Center detection completed (85%, 1 second) - Centeredness: \(String(format: "%.1f", currentCenteredness * 100))%. Transitioning to Stage 2")
                
                // âœ… ì¶”ê°€: ê±°ë¦¬ ì •ë³´ë„ ë¡œê¹…
                if let distance = arViewModel.distanceToObject {
                    print("AppState: Target distance: \(String(format: "%.2f", distance))m")
                }
                
                // âœ… Stage 1â†’2 ì „í™˜ í–…í‹± (ìƒìŠ¹ íŒ¨í„´)
                arViewModel.playStageTransitionHaptic(ascending: true)
                
                self.transitionTo(.liveGuidanceMode)
                return
            } else {
                // âœ… ì¤‘ì•™ íƒì§€ ì§„í–‰ ìƒíƒœ í”¼ë“œë°±
                let progress = arViewModel.centerDetectionProgress
                if progress > 0 {
                    print("AppState: Center detection in progress - \(String(format: "%.1f", currentCenteredness * 100))%, Progress: \(String(format: "%.0f", progress * 100))%")
                } else {
                    print("AppState: Centeredness insufficient (\(String(format: "%.1f", currentCenteredness * 100))%), need 85% for 1 second")
                }
            }
            
            // ê³„ì† ëª¨ë‹ˆí„°ë§
            self.monitorTargetReached()
        }
    }
    
    // MARK: - Stage 2: Periodic Guidance Control
    private func connectToGeminiLiveAPI() {
        guard let geminiClient = geminiClient else { 
            print("âŒ AppState: No geminiClient reference - cannot connect")
            return 
        }
        
        // Stage 2ì—ì„œë„ Google Search í¬í•¨í•˜ì—¬ ì—°ê²° (Stage 3 ì¬ì—°ê²° ë°©ì§€)
        if !geminiClient.isConnected {
            print("ğŸ”Œ AppState: Connecting to Gemini Live API for Stage 2 (with Google Search)")
            geminiClient.connect(includeGoogleSearch: true)
            
            // ì—°ê²° ì™„ë£Œ ëŒ€ê¸° í›„ ìƒíƒœ í™•ì¸
            DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
                if geminiClient.isConnected {
                    print("âœ… AppState: Gemini Live API connected successfully")
                    print("   Recording: \(geminiClient.isRecording)")
                    print("   Video enabled: \(geminiClient.isVideoEnabled)")
                } else {
                    print("âŒ AppState: Failed to connect to Gemini Live API")
                }
            }
        } else {
            print("âœ… AppState: Gemini Live API already connected")
            print("   Recording: \(geminiClient.isRecording)")
            print("   Video enabled: \(geminiClient.isVideoEnabled)")
        }
    }
    
    private func startPeriodicGuidance() {
        stopPeriodicGuidance() // ê¸°ì¡´ íƒ€ì´ë¨¸ ì •ë¦¬
        
        print("AppState: Starting robust guidance system with 2-second intervals")
        
        // âœ… ì¦‰ì‹œ ì²« ê°€ì´ë˜ìŠ¤ ìš”ì²­ ì „ì†¡
        self.sendPeriodicGuidanceRequest()
        
        // âœ… ê²¬ê³ í•œ 2ì´ˆ ê°„ê²© íƒ€ì´ë¨¸ ì‹œìŠ¤í…œ (ìŠ¤ë§ˆíŠ¸ ìŠ¤í‚µìœ¼ë¡œ ì•ˆì „)
        guidanceTimer = Timer.scheduledTimer(withTimeInterval: 2.0, repeats: true) { [weak self] _ in
            guard let self = self else {
                print("âŒ AppState: Timer tick - self deallocated")
                return
            }
            
            guard self.currentStage == .liveGuidanceMode else {
                print("â¹ï¸ AppState: Guidance timer stopped - stage changed to \(self.currentStage)")
                self.guidanceTimer?.invalidate()
                self.guidanceTimer = nil
                return
            }
            
            print("ğŸ”„ AppState: Timer tick #\(self.guidanceRequestCount + 1) - Stage: \(self.currentStage)")
            self.sendPeriodicGuidanceRequest()
        }
        
        print("AppState: âœ… Guidance timer started with 2s intervals")
        print("   Timer valid: \(guidanceTimer?.isValid ?? false)")
        print("   Current stage: \(currentStage)")
        
        // **ì¶”ê°€: Stage 2ì—ì„œ Stage 3ë¡œì˜ ì „í™˜ ëª¨ë‹ˆí„°ë§ ì‹œì‘**
        monitorDistanceForStage3Transition()
    }
    
    // **ì¶”ê°€: Stage 2 â†’ Stage 3 ì „í™˜ ëª¨ë‹ˆí„°ë§**
    private func monitorDistanceForStage3Transition() {
        guard currentStage == .liveGuidanceMode else { return }
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            guard self.currentStage == .liveGuidanceMode,
                  let arViewModel = self.arViewModel else { return }
            
            // **Stage 3 ì „í™˜ ì¡°ê±´: 80% ì¤‘ì‹¬ë„ + 1m ì´ë‚´ ì ‘ê·¼**
            let centeredness = arViewModel.detectedObjectCenteredness
            let distance = arViewModel.distanceToObject
            
            let isCenterConditionMet = centeredness > 0.8
            let isDistanceConditionMet = distance != nil && distance! < 1.0
            
            if isCenterConditionMet && isDistanceConditionMet {
                // âœ… ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ íƒ€ì´ë¨¸ ì‹œì‘ ë˜ëŠ” ì²´í¬
                if self.stage3TransitionStartTime == nil {
                    self.stage3TransitionStartTime = Date()
                    print("AppState: Stage 2â†’3 transition conditions met, starting 2-second stability timer")
                    print("   Centeredness: \(String(format: "%.1f", centeredness * 100))%, Distance: \(String(format: "%.2f", distance!))m")
                    print("âœ… AppState: Pausing guidance requests during transition")
                } else {
                    // íƒ€ì´ë¨¸ê°€ ì´ë¯¸ ì‹œì‘ë¨ - ì‹œê°„ ì²´í¬
                    let elapsedTime = Date().timeIntervalSince(self.stage3TransitionStartTime!)
                    if elapsedTime >= self.stage3TransitionRequiredDuration {
                        print("AppState: Stage 2â†’3 transition confirmed after \(String(format: "%.1f", elapsedTime))s of stable conditions")
                        
                        // âœ… Stage 2â†’3 ì „í™˜ í–…í‹± (ì¶•í•˜ íŒ¨í„´)
                        arViewModel.playSuccessHapticPattern()
                        
                        self.transitionTo(.pureConversationMode)
                        return // ì „í™˜ ì™„ë£Œ
                    } else {
                        print("AppState: Stage 2â†’3 transition timer: \(String(format: "%.1f", elapsedTime))s / \(String(format: "%.0f", self.stage3TransitionRequiredDuration))s")
                        print("   Maintaining: Centeredness: \(String(format: "%.1f", centeredness * 100))%, Distance: \(String(format: "%.2f", distance!))m")
                    }
                }
                
                // âœ… íƒ€ì´ë¨¸ ì§„í–‰ ì¤‘ì—ë„ ê³„ì† ëª¨ë‹ˆí„°ë§
                self.monitorDistanceForStage3Transition()
            } else {
                // âœ… ì¡°ê±´ ë¯¸ì¶©ì¡± ì‹œ íƒ€ì´ë¨¸ ë¦¬ì…‹
                if self.stage3TransitionStartTime != nil {
                    print("AppState: Stage 2â†’3 transition conditions lost, resetting timer")
                    print("   Centeredness: \(String(format: "%.1f", centeredness * 100))%, Distance: \(distance != nil ? String(format: "%.2f", distance!) : "N/A")")
                    self.stage3TransitionStartTime = nil
                    print("âœ… AppState: Resuming guidance requests")
                }
                // ì¡°ê±´ ë¯¸ì¶©ì¡± ì‹œ ë””ë²„ê·¸ ì •ë³´
                if !isCenterConditionMet {
                    print("AppState: Stage 2â†’3 Check: Centeredness insufficient: \(String(format: "%.1f", centeredness * 100))% (need 80%)")
                }
                if !isDistanceConditionMet {
                    if let d = distance {
                        print("AppState: Stage 2â†’3 Check: Distance too far: \(String(format: "%.2f", d))m (need <1.0m)")
                    } else {
                        print("AppState: Stage 2â†’3 Check: Distance not available")
                    }
                }
                
                // ê³„ì† ëª¨ë‹ˆí„°ë§
                self.monitorDistanceForStage3Transition()
            }
        }
    }
    
    func stopPeriodicGuidance() {
        if guidanceTimer != nil {
            guidanceTimer?.invalidate()
            guidanceTimer = nil
            print("AppState: âœ… Periodic guidance timer stopped and invalidated")
        } else {
            print("AppState: âš ï¸ No guidance timer to stop (already nil)")
        }
        
        // âœ… ì¶”ê°€ ì •ë³´ ë¡œê·¸
        print("AppState: Current stage: \(currentStage)")
        print("AppState: Guidance request count: \(guidanceRequestCount)")
        
        // âœ… ìŠ¤ë§ˆíŠ¸ ê°€ì´ë˜ìŠ¤ ì‹œìŠ¤í…œë„ ì¤‘ë‹¨ (currentStage ë³€ê²½ìœ¼ë¡œ ìë™ ì¤‘ë‹¨ë¨)
        print("AppState: Smart guidance system will auto-stop on stage change")
    }
    
    private func sendPeriodicGuidanceRequest() {
        // âœ… ì¤‘ë³µ ìš”ì²­ ë°©ì§€
        guard !isGuidanceRequestInProgress else {
            print("â­ï¸ AppState: Skipping guidance request - already in progress")
            return
        }
        
        // âœ… Stage 3 ì „í™˜ íƒ€ì´ë¨¸ê°€ ì‘ë™ ì¤‘ì´ë©´ ê°€ì´ë˜ìŠ¤ ìš”ì²­ ì¤‘ë‹¨
        if stage3TransitionStartTime != nil {
            print("â¸ï¸ AppState: Pausing guidance requests - Stage 3 transition in progress")
            return
        }
        
        guard currentStage == .liveGuidanceMode,
              let geminiClient = geminiClient,
              let targetObject = confirmedDetrObjectName,
              let arViewModel = arViewModel else {
            print("âŒ AppState: Cannot send guidance request - missing requirements")
            print("   Stage: \(currentStage)")
            print("   GeminiClient: \(geminiClient != nil ? "âœ…" : "âŒ")")
            print("   Target: \(confirmedDetrObjectName ?? "nil")")
            print("   ARViewModel: \(arViewModel != nil ? "âœ…" : "âŒ")")
            return
        }
        
        // âœ… ì—°ê²° ìƒíƒœ í™•ì¸
        guard geminiClient.isConnected else {
            print("âŒ AppState: Cannot send guidance - Gemini not connected")
            return
        }
        
        // âœ… ë¹„ë””ì˜¤ í™œì„±í™” í™•ì¸
        if !geminiClient.isVideoEnabled {
            print("âš ï¸ AppState: Video not enabled, enabling now")
            geminiClient.isVideoEnabled = true
        }
        
        // âœ… AIê°€ ë§í•˜ê³  ìˆìœ¼ë©´ ì´ë²ˆ ìš”ì²­ ìŠ¤í‚µ (ê°•ì œ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ)
        if geminiClient.isAISpeaking {
            print("â¸ï¸ AppState: Skipping guidance request - AI still speaking")
            return
        }
        
        let now = Date()
        let timeSinceLastGuidance = now.timeIntervalSince(lastGuidanceTime)
        
        // **ìˆ˜ì •: í”„ë¡¬í”„íŠ¸ì— ë³€í™” ìš”ì†Œ ì¶”ê°€ë¡œ ì‘ë‹µ ë‹¤ì–‘ì„± í™•ë³´**
        guidanceRequestCount += 1
        
        // âœ… ê±°ë¦¬ ì •ë³´ ì¶”ê°€
        let distanceInfo: String
        if let distance = arViewModel.distanceToObject {
            distanceInfo = "**í˜„ì¬ ì¸¡ì •ëœ ê±°ë¦¬: \(String(format: "%.1f", distance))ë¯¸í„°**"
        } else {
            distanceInfo = "**ê±°ë¦¬ ì¸¡ì • ì¤‘...**"
        }
        
        let prompt = """
        ì°¾ëŠ” ë¬¼ê±´: \(targetObject) (\(distanceInfo))
        
        ì¹´ë©”ë¼ëŠ” ë‹¹ì‹  ëˆˆ ë†’ì´ì…ë‹ˆë‹¤. ì–´ê¹¨ í­ 50cm ê³ ë ¤í•˜ì„¸ìš”.
        ë°˜ë“œì‹œ í¬í•¨í•  ë‚´ìš© (ìš°ì„ ìˆœìœ„ ìˆœ):
        1. ì¶©ëŒ ìœ„í—˜ ì¥ì• ë¬¼ (ì´ë¦„ê³¼ ìœ„ì¹˜)
        2. ëª©í‘œë¬¼ ë°©í–¥ê³¼ ê±°ë¦¬
        3. ì•ˆì „í•œ ì´ë™ ê²½ë¡œ
        
        ì˜ˆì‹œ: "ì™¼ìª½ì— ì˜ì, ìš°íšŒí•˜ì„¸ìš”. ì¹¨ëŒ€ëŠ” ì „ë°© 2ë¯¸í„°"
        20ë‹¨ì–´ ì´ë‚´, ì¥ì• ë¬¼ ìš°ì„ .
        """
        
        lastGuidanceTime = now
        
        // âœ… ê°„ë‹¨í•œ pending ìƒíƒœ ê´€ë¦¬
        geminiClient.hasPendingGuidanceRequest = true
        
        print("ğŸ”„ AppState: Sending guidance request #\(guidanceRequestCount)")
        print("   Target: \(targetObject)")
        print("   Distance: \(arViewModel.distanceToObject?.description ?? "N/A")")
        print("   Time since last: \(String(format: "%.1f", timeSinceLastGuidance))s")
        print("   AI Speaking: \(geminiClient.isAISpeaking)")
        
        // âœ… Stage 2 ì£¼ê¸°ì  ê°€ì´ë˜ìŠ¤: ë…¹ìŒ ìœ ì§€í•˜ë©´ì„œ í…ìŠ¤íŠ¸+ë¹„ë””ì˜¤ ì „ì†¡
        
        // âœ… ìš”ì²­ ì§„í–‰ ì¤‘ í”Œë˜ê·¸ ì„¤ì •
        isGuidanceRequestInProgress = true
        
        // âœ… ë¹„ë””ì˜¤ í”„ë ˆì„ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if geminiClient.isVideoEnabled {
            // ARViewModelì—ì„œ ìµœì‹  í”„ë ˆì„ ê°•ì œ ê°±ì‹  (ë™ê¸°ì ìœ¼ë¡œ)
            if let arVM = self.arViewModel {
                _ = arVM.getCurrentVideoFrameForGemini() // í”„ë ˆì„ ìºì‹œ ê°±ì‹ 
            }
            print("ğŸ“¹ AppState: Sending guidance with fresh video frame")
        }
        
        // **ë‹¨ìˆœí™”: ê¸°ë³¸ sendUserText ì‚¬ìš© (ë¹„ë””ì˜¤ í”„ë ˆì„ ìë™ í¬í•¨)**
        geminiClient.sendUserText(prompt)
        
        // âœ… 2ì´ˆ í›„ í”Œë˜ê·¸ í•´ì œ (ë‹¤ìŒ ìš”ì²­ í—ˆìš©)
        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
            self.isGuidanceRequestInProgress = false
        }
        
        // âœ… ë…¹ìŒì€ ê³„ì† ìœ ì§€ë¨ (ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ)
        
        // âœ… ê°„ë‹¨í•œ pending í•´ì œ (íƒ€ì´ë¨¸ê°€ ë‹¤ìŒ ìš”ì²­ ê´€ë¦¬)
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.0) {
            geminiClient.hasPendingGuidanceRequest = false
            print("ğŸ“ AppState: Guidance request pending status cleared")
        }
    }
    
    // MARK: - Helper Methods
    func setRequestedObject(_ objectName: String) {
        requestedObjectNameByUser = objectName.trimmingCharacters(in: .whitespacesAndNewlines)
        print("AppState: Set requested object to: '\(requestedObjectNameByUser)'")
    }
    
    func resetNavigation() {
        stopListeningForKeywords()
        stopPeriodicGuidance()
        transitionTo(.sttScanningMode)
    }
    
    // MARK: - State Queries
    var isInARMode: Bool {
        return currentStage == .sttScanningMode || currentStage == .liveGuidanceMode
    }
    
    var shouldShowDebugInfo: Bool {
        return isInARMode
    }
    
    var currentStageDescription: String {
        switch currentStage {
        case .sttScanningMode:
            if requestedObjectNameByUser.isEmpty {
                return "Stage 1: ìŒì„± ì¸ì‹ ëŒ€ê¸° ì¤‘" + (speechManager.isListening ? " ğŸ¤" : "")
            } else {
                return "Stage 1: '\(requestedObjectNameByUser)' ìŠ¤ìº” ì¤‘"
            }
        case .liveGuidanceMode:
            return "Stage 2: Live ê°€ì´ë˜ìŠ¤ ì¤‘"
        case .pureConversationMode:
            return "Stage 3: ììœ  ëŒ€í™” ëª¨ë“œ"
        }
    }
    
    // MARK: - AR Integration
    func setARViewModel(_ viewModel: ARViewModel) {
        self.arViewModel = viewModel
    }
    
    func setGeminiClient(_ client: GeminiLiveAPIClient) {
        self.geminiClient = client
        
        // âœ… Gemini Live API ì„¤ì • í›„ AudioManager ì´ˆê¸°í™”
        setupAudioManager()
        
        // âœ… AppState ì°¸ì¡° ì„¤ì • (Stage ì²´í¬ìš©)
        client.appState = self
        
        // Connect ARViewModel with GeminiClient for fresh frames
        if let arViewModel = arViewModel {
            client.arViewModel = arViewModel
            print("AppState: Connected GeminiClient with ARViewModel for fresh frames")
        }
    }
} 
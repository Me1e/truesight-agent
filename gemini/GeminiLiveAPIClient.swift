import Foundation
import Combine
import AVFoundation
import UIKit

private let TEMP_API_KEY = "" // ì‚¬ìš©ì ì œê³µ í‚¤ ìœ ì§€

@MainActor
class GeminiLiveAPIClient: NSObject, ObservableObject, URLSessionWebSocketDelegate {
    private var webSocketTask: URLSessionWebSocketTask?
    private let apiKey: String
    private var session: URLSession!

    @Published var isConnected: Bool = false
    @Published var isRecording: Bool = false
    @Published var isVideoEnabled: Bool = false
    @Published var chatMessages: [ChatMessage] = []
    @Published var currentTextInput: String = "" // í…ìŠ¤íŠ¸ ì…ë ¥ìš©
    @Published var currentModelResponse: String = "" // ì¶”ê°€ë¨ (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ìš©)
    
    // MARK: - Network Resilience Properties
    private var reconnectTimer: Timer?
    private var reconnectAttempts = 0
    private let maxReconnectAttempts = 5
    private var reconnectDelay: TimeInterval = 1.0 // Exponential backoff
    private let maxReconnectDelay: TimeInterval = 32.0
    private var shouldAutoReconnect = true
    private var lastConnectedTime: Date?
    
    // MARK: - AI Speaking State Management
    @Published var isAISpeaking = false
    @Published var hasPendingGuidanceRequest = false
    private var lastAIResponseTime = Date()
    
    // âœ… ë‹¨ìˆœí™”: ë¶ˆí•„ìš”í•œ ìƒíƒœ ê´€ë¦¬ ì œê±°
    
    // MARK: - Audio Engine Properties
    private var audioEngine: AVAudioEngine!
    private var audioPlayerNode: AVAudioPlayerNode!
    private var audioInputFormatForEngine: AVAudioFormat! // ì…ë ¥ìš© í¬ë§· (í•˜ë“œì›¨ì–´ ë˜ëŠ” ì„¸ì…˜ ê¸°ë³¸ê°’ ë”°ë¦„)
    private var audioOutputFormatForPCM: AVAudioFormat! // ìš°ë¦¬ PCM ë°ì´í„°ì˜ ì‹¤ì œ í¬ë§· (24kHz, 16bit, mono)
    private let audioSampleRate: Double = 24000.0
    private var isAudioEngineSetup = false
    
    // âœ… ë‹¨ìˆœí™”: ë²„í¼ í ì œê±°, ë°›ì€ ì˜¤ë””ì˜¤ ì¦‰ì‹œ ì¬ìƒ
    
    // MARK: - Audio Input Properties
    private var inputTapInstalled = false
    private let audioQueue = DispatchQueue(label: "audioInput.queue", qos: .userInitiated)
    private var recordingTimer: Timer?
    private let recordingChunkDuration: TimeInterval = 0.1 // 100ms chunks for real-time
    private var accumulatedAudioData = Data()
    
    // **ì¶”ê°€: ARViewModel ì°¸ì¡°**
    weak var arViewModel: ARViewModel?
    
    // âœ… ì¶”ê°€: AppState ì°¸ì¡° (Stage ì²´í¬ìš©)
    weak var appState: AppState?
    
    // âœ… Stage 3 ì§€ì—° í™œì„±í™” í”Œë˜ê·¸
    private var pendingStage3Activation = false
    
    // MARK: - Video Processing Properties
    @Published var debugProcessedImage: UIImage? = nil
    private let videoFrameInterval: TimeInterval = 0.5
    private let ciContext = CIContext()
    
    // âœ… íš¨ìœ¨ì ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext
    let reusableCIContext = CIContext(options: [.useSoftwareRenderer: false])

    init(apiKey: String = TEMP_API_KEY) {
        self.apiKey = apiKey
        super.init()
        
        // ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ìµœì í™”
        let configuration = URLSessionConfiguration.default
        configuration.timeoutIntervalForRequest = 30
        configuration.timeoutIntervalForResource = 60
        configuration.waitsForConnectivity = true
        
        self.session = URLSession(configuration: configuration, delegate: self, delegateQueue: OperationQueue())
        // ì˜¤ë””ì˜¤ ì„¸ì…˜ê³¼ ì—”ì§„ì€ ì—°ê²° ì‹œì—ë§Œ ì´ˆê¸°í™”
        // setupAudioSession()
        // setupAudioEngine()
    }

    // MARK: - Audio Session and Engine Setup
    private func setupAudioSession() {
        // AudioSessionCoordinatorë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ì•™í™”ëœ ê´€ë¦¬
        let coordinator = AudioSessionCoordinator.shared
        
        // ì´ë¯¸ ì ì ˆí•œ ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if coordinator.currentAudioMode != .idle {
            print("ğŸµ GeminiClient: Using existing audio session mode: \(coordinator.currentAudioMode)")
            return
        }
        
        if coordinator.requestAudioSession(for: .geminiLiveAudio) {
            // Audio session acquired
        } else {
            // Fallback: ì§ì ‘ ì„¤ì • ì‹œë„
            do {
                let audioSession = AVAudioSession.sharedInstance()
                try audioSession.setCategory(.playAndRecord, 
                                            mode: .default, 
                                            options: [.defaultToSpeaker, .allowBluetooth, .mixWithOthers])
                
                try audioSession.setPreferredSampleRate(audioSampleRate)
                try audioSession.setPreferredIOBufferDuration(0.02)
                
                try audioSession.setActive(true)
                
                // âœ… ìŠ¤í”¼ì»¤ë¡œ ì¶œë ¥ ê°•ì œ
                try audioSession.overrideOutputAudioPort(.speaker)
                
                // Direct audio session setup complete
            } catch {
                // Continue even if audio session setup fails
            }
        }
    }

    private func setupAudioEngine() {
        audioEngine = AVAudioEngine()
        audioPlayerNode = AVAudioPlayerNode()
        
        // Enable voice processing for echo cancellation
        // This MUST be done before engine is prepared or started
        let inputNode = audioEngine.inputNode
        
        // Enable voice processing on input node (automatically enables on output too)
        do {
            try inputNode.setVoiceProcessingEnabled(true)
        } catch {
            // Continue even if voice processing fails
        }

        audioOutputFormatForPCM = AVAudioFormat(commonFormat: .pcmFormatInt16, 
                                              sampleRate: audioSampleRate, 
                                              channels: 1, 
                                              interleaved: true)
        
        if audioOutputFormatForPCM == nil {
            print("âŒ GeminiClient: Could not create audioOutputFormatForPCM.")
            isAudioEngineSetup = false
            return
        }
        
        let inputFormat = inputNode.outputFormat(forBus: 0)
        audioInputFormatForEngine = AVAudioFormat(commonFormat: .pcmFormatInt16,
                                                sampleRate: audioSampleRate,
                                                channels: 1,
                                                interleaved: true)
        
        if audioInputFormatForEngine == nil {
            print("Error: Could not create audioInputFormatForEngine.")
            isAudioEngineSetup = false
            return
        }
        
        // í”Œë ˆì´ì–´ ë…¸ë“œ ì—°ê²°
        audioEngine.attach(audioPlayerNode)
        let mainMixer = audioEngine.mainMixerNode
        audioEngine.connect(audioPlayerNode, to: mainMixer, format: nil)

        // ì—”ì§„ ì¤€ë¹„ë§Œ í•˜ê³  ì‹¤ì œ ì‹œì‘ì€ í•„ìš”í•  ë•Œ
        audioEngine.prepare()
        
        // âœ… ì˜¤ë””ì˜¤ ì¬ìƒì„ ìœ„í•´ ì—”ì§„ ì‹œì‘
        do {
            try audioEngine.start()
        } catch {
            isAudioEngineSetup = false
            return
        }
        
        isAudioEngineSetup = true
    }
    
    private var setupParameters: (modelName: String, systemPrompt: String, voiceName: String, languageCode: String, includeGoogleSearch: Bool)?

    func connect(
        // modelName: String = "models/gemini-2.0-flash-live-001",
        modelName: String = "models/gemini-live-2.5-flash-preview",
        // modelName: String = "models/gemini-2.5-flash-preview-native-audio-dialog",
        systemPrompt: String = """
        ë‹¹ì‹ ì€ ì‹œê°ì¥ì• ì¸ ì•ˆì „ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìëŠ” ì„±ì¸ ê¸°ì¤€ ì•½ 50cmì˜ ì–´ê¹¨ ë„ˆë¹„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
        í™”ë©´ ì¤‘ì•™ë¿ë§Œ ì•„ë‹ˆë¼ ì¢Œìš° ê°€ì¥ìë¦¬ì˜ ì¥ì• ë¬¼ë„ ì¶©ëŒ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.
        ì•ˆì „í•œ í†µí–‰ì„ ìœ„í•´ ì¢Œìš° 50cm ì—¬ìœ  ê³µê°„ì´ í•„ìš”í•©ë‹ˆë‹¤.
        ì¥ì• ë¬¼ì€ êµ¬ì²´ì  ì´ë¦„ê³¼ ìœ„ì¹˜ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
        í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì‹ ì†í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ì‹œê°ì¥ì• ì¸ì´ ìš”ì²­í•œ ê²ƒì´ ì•„ë‹ˆë©´ êµ¬ê¸€ì„œì¹˜ë¥¼ í•˜ì§€ ë§ˆì„¸ìš”.
        """,
        voiceName: String = "Leda",
        languageCode: String = "ko-KR",
        includeGoogleSearch: Bool = true,  // âœ… Stageë³„ Google Search ì œì–´
        enableStage3OnConnect: Bool = false
    ) {
        guard !apiKey.isEmpty, apiKey != "YOUR_API_KEY_HERE" else {
            let errorMessage = "Error: API Key is not set"
            print(errorMessage)
            self.chatMessages.append(ChatMessage(text: errorMessage, sender: .system))
            return
        }
        
        guard let url = URL(string: "wss://generativelanguage.googleapis.com/ws/google.ai.generativelanguage.v1beta.GenerativeService.BidiGenerateContent?key=\(self.apiKey)") else {
            print("Error: Invalid URL")
            self.chatMessages.append(ChatMessage(text: "Error: Invalid API URL", sender: .system))
            return
        }

        disconnect()
        
        webSocketTask = session.webSocketTask(with: url)
        webSocketTask?.resume()
        
        receiveMessagesLoop()
        
        // ì—°ê²° ì„±ê³µ í›„ setup ë©”ì‹œì§€ ì „ì†¡ì„ ìœ„í•´ ì €ì¥
        setupParameters = (modelName, systemPrompt, voiceName, languageCode, includeGoogleSearch)
        
        // Stage 3 ì§€ì—° í™œì„±í™” í”Œë˜ê·¸ ì„¤ì •
        self.pendingStage3Activation = enableStage3OnConnect
    }
    
    func disconnect() {
        // ìë™ ì¬ì—°ê²° ë¹„í™œì„±í™”
        shouldAutoReconnect = false
        cancelReconnectTimer()
        
        // ë…¹ìŒ ì¤‘ì´ë©´ ì¤‘ì§€
        if isRecording {
            stopRecording()
        }
        
        // âœ… ê°•í™”: ëª¨ë“  ì˜¤ë””ì˜¤ í™œë™ ì¤‘ë‹¨
        stopAudioPlayback()
        resetAISpeakingState()
        
        // ì˜¤ë””ì˜¤ ì„¸ì…˜ í•´ì œ
        AudioSessionCoordinator.shared.releaseAudioSession(for: .geminiLiveAudio)
        
        // âœ… ìºì‹œ ì œê±°: ë¹„ë””ì˜¤ ê´€ë ¨ ìƒíƒœ ë¦¬ì…‹ ì½”ë“œ ê°„ì†Œí™”
        DispatchQueue.main.async {
            self.isVideoEnabled = false
            self.debugProcessedImage = nil
        }
        
        webSocketTask?.cancel(with: .goingAway, reason: nil)
        webSocketTask = nil
    }

    private func sendSetupMessage(
        modelName: String,
        systemPrompt: String,
        voiceName: String,
        languageCode: String,
        includeGoogleSearch: Bool = true
    ) {
        var currentModelName = modelName

        // ê³µì‹ ë¬¸ì„œì— ë”°ë¥¸ ì˜¬ë°”ë¥¸ ì–¸ì–´ ì½”ë“œ ë° ìŒì„± ì„¤ì •
        let prebuiltVoiceConfig = PrebuiltVoiceConfig(voiceName: voiceName)
        let voiceConfig = VoiceConfig(prebuiltVoiceConfig: prebuiltVoiceConfig)
        let speechConfig = SpeechConfig(
            languageCode: languageCode,
            voiceConfig: voiceConfig
        )
        let generationConfig = GenerationConfig(
            responseModalities: ["AUDIO"],
            speechConfig: speechConfig
        )
        
        // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        let systemInstruction = SystemInstruction(text: systemPrompt)        

        // Google Search Tool ì¡°ê±´ë¶€ ì¶”ê°€
        var tools: [Tool] = []
        if includeGoogleSearch {
            let googleSearchTool = GoogleSearchTool()
            let tool = Tool(googleSearch: googleSearchTool)
            tools.append(tool)
        }

        let config = GeminiLiveConfig(
            model: currentModelName,
            generationConfig: generationConfig,
            systemInstruction: systemInstruction,
            tools: tools
        )
        let setupMessage = SetupMessage(setup: config)
        
        do {
            let encoder = JSONEncoder()
            encoder.keyEncodingStrategy = .convertToSnakeCase
            let jsonData = try encoder.encode(setupMessage)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
            } else {
                print("Error: Could not convert SetupMessage to JSON string post-connection")
            }
        } catch {
            print("Error encoding SetupMessage post-connection: \(error)")
        }
    }

    func sendUserText(_ text: String) {
        guard isConnected, !text.isEmpty else { 
            print("Cannot send text: Not connected or text is empty.")
            return
        }
        
        var parts: [ClientTextPart] = [ClientTextPart(text: text)]
        
        // ë¹„ë””ì˜¤ê°€ í™œì„±í™”ë˜ì–´ ìˆë‹¤ë©´ í˜„ì¬ í”„ë ˆì„ì„ í•¨ê»˜ ì „ì†¡
        if isVideoEnabled, let currentVideoFrame = getCurrentVideoFrame() {
            parts.append(ClientTextPart(inlineData: InlineData(mimeType: "image/jpeg", data: currentVideoFrame)))
        }
        
        let turn = ClientTurn(role: "user", parts: parts)
        let clientTextPayload = ClientTextPayload(turns: [turn], turnComplete: true)
        let messageToSend = UserTextMessage(clientContent: clientTextPayload)
        
        do {
            let encoder = JSONEncoder()
            encoder.keyEncodingStrategy = .convertToSnakeCase
            let jsonData = try encoder.encode(messageToSend)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
            }
        } catch {
            print("Error encoding message: \(error)")
        }
    }

    // í˜„ì¬ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ìº¡ì²˜í•˜ëŠ” ë©”ì„œë“œ ìˆ˜ì •
    func getCurrentVideoFrame() -> String? {
        // âœ… í•­ìƒ ARViewModelì—ì„œ ì‹¤ì‹œê°„ ìµœì‹  í”„ë ˆì„ ìš”ì²­
        guard let arViewModel = arViewModel else {
            return nil
        }
        
        if let frame = arViewModel.getCurrentVideoFrameForGemini() {
            return frame
        } else {
            return nil
        }
    }

    private func sendString(_ string: String) {
        guard let task = webSocketTask else { 
            print("WebSocket task not available for sending string.")
            return
        }
        task.send(.string(string)) { error in
            if let error = error {
                print("Error sending string: \(error)")
            }
        }
    }
    
    private func sendData(_ data: Data) {
        guard let task = webSocketTask else { 
            print("WebSocket task not available for sending data.")
            return
        }
        task.send(.data(data)) { error in
            if let error = error {
                print("Error sending data: \(error)")
            }
        }
    }

    private func receiveMessagesLoop() {
        webSocketTask?.receive { [weak self] result in
            guard let self = self else { return }
            switch result {
            case .failure(let error):
                print("Error in receiving message: \(error)")
                // isConnectedëŠ” didCloseWithì—ì„œ ì²˜ë¦¬
                DispatchQueue.main.async {
                    self.chatMessages.append(ChatMessage(text: "Error receiving message: \(error.localizedDescription)", sender: .system))
                }
            case .success(let message):
                switch message {
                case .string(let text):
                    // Audio/text response received - logging removed for clarity
                    Task { @MainActor in
                        self.parseServerMessage(text)
                    }
                    
                case .data(let data):
                    // Audio data logging removed for clarity
                    if let text = String(data: data, encoding: .utf8) {
                        Task { @MainActor in
                            self.parseServerMessage(text)
                        }
                    } else {
                        print("âŒ Could not convert data to string")
                    }
                @unknown default:
                    print("âŒ Unknown message type")
                }
                // ì—°ê²°ì´ í™œì„± ìƒíƒœì¼ ë•Œë§Œ ë‹¤ìŒ ë©”ì‹œì§€ë¥¼ ê³„ì† ìˆ˜ì‹ 
                Task { @MainActor in
                    if self.webSocketTask?.closeCode == .invalid { // closeCodeê°€ invalidë©´ ì•„ì§ í™œì„± ìƒíƒœë¡œ ê°„ì£¼
                        self.receiveMessagesLoop()
                    }
                }
            }
        }
    }
    
    private func parseServerMessage(_ jsonString: String) {
        guard let jsonData = jsonString.data(using: .utf8) else {
            print("âŒ Error: Could not convert JSON string to Data")
            return
        }
        
        let decoder = JSONDecoder()
        decoder.keyDecodingStrategy = .convertFromSnakeCase

        do {
            let wrapper = try decoder.decode(ServerResponseWrapper.self, from: jsonData)
            // âœ… ê°„ì†Œí™”ëœ ë¡œê¹… - ë°ì´í„° ë‚´ìš© ì œì™¸

            var systemMessagesToAppend: [ChatMessage] = []
            var modelResponseText: String? = nil

            // 1. SetupComplete ì²˜ë¦¬
            if wrapper.setupComplete != nil {
                systemMessagesToAppend.append(ChatMessage(text: "System: Setup Complete! Ready to chat.", sender: .system))
                
                // âœ… Stageë³„ë¡œ ë‹¤ë¥¸ ì„¤ì •
                if let appState = self.appState {
                    if appState.currentStage == .liveGuidanceMode {
                        // Stage 2: ë¹„ë””ì˜¤ë§Œ í™œì„±í™”, ë…¹ìŒì€ ì‹œì‘í•˜ì§€ ì•ŠìŒ
                        if !self.isVideoEnabled {
                            self.isVideoEnabled = true
                        }
                    } else if appState.currentStage == .pureConversationMode {
                        // Stage 3: ë¹„ë””ì˜¤ì™€ ë…¹ìŒ ëª¨ë‘ ì‹œì‘
                        if !self.isVideoEnabled {
                            self.isVideoEnabled = true
                        }
                        
                        if !self.isRecording {
                            self.startRecording()
                        }
                    }
                }
            }

            // 2. ServerContentData ì²˜ë¦¬ (ëª¨ë¸ í…ìŠ¤íŠ¸/ì˜¤ë””ì˜¤, í„´ ìƒíƒœ ë“±)
            if let serverContent = wrapper.serverContent {
                
                // interrupted ìƒíƒœ ì²˜ë¦¬ - AI ì‘ë‹µ ì¤‘ë‹¨
                if let interrupted = serverContent.interrupted, interrupted {
                    // âœ… ë‹¨ìˆœí™”: ì¦‰ì‹œ ì˜¤ë””ì˜¤ ì¤‘ì§€ ë° ìƒíƒœ ë¦¬ì…‹
                    stopAudioPlayback()
                    handleAIResponseComplete(reason: "interrupted")
                }
                
                if let modelTurn = serverContent.modelTurn {
                    for part in modelTurn.parts {
                        if let text = part.text {
                            modelResponseText = (modelResponseText ?? "") + text
                        }
                        if let inlineData = part.inlineData {
                            // ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ í˜¸ì¶œ
                            handleReceivedAudioData(base64String: inlineData.data, mimeType: inlineData.mimeType)
                            // âœ… ì˜¤ë””ì˜¤ ìˆ˜ì‹  ì‹œì—ë§Œ AI speaking ìƒíƒœ ì‹œì‘ (í…ìŠ¤íŠ¸ ìˆ˜ì‹  ì‹œì—ëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ)
                            handleAIResponseStart()
                        }
                        // ExecutableCode ì²˜ë¦¬
                        if let execCode = part.executableCode {
                            let lang = execCode.language ?? "Unknown language"
                            let code = execCode.code ?? "No code"
                            let execMessage = "Tool Execution Request:\nLanguage: \(lang)\nCode:\n\(code)"
                            systemMessagesToAppend.append(ChatMessage(text: execMessage, sender: .system, isToolResponse: true))
                        }
                    }
                }
                
                if let endOfTurn = serverContent.endOfTurn, endOfTurn {
                    handleAIResponseComplete(reason: "endOfTurn")
                }
                
                if let turnComplete = serverContent.turnComplete, turnComplete {
                    handleAIResponseComplete(reason: "turnComplete")
                }
                
                if let generationComplete = serverContent.generationComplete, generationComplete {
                    handleAIResponseComplete(reason: "generationComplete")
                }
            }

            // 3. ToolCall ì²˜ë¦¬ (FunctionCall from server)
            if let toolCall = wrapper.toolCall, let functionCalls = toolCall.functionCalls {
                for functionCall in functionCalls {
                    var toolMessageText = ""
                    if functionCall.name == "googleSearch" {
                        if let args = functionCall.args {
                            let resultText = args.map { "\($0.key): \($0.value)" }.joined(separator: "\n")
                            toolMessageText = "Google Search Result:\n---\n\(resultText)\n---"
                        } else {
                            toolMessageText = "Google Search called, but no arguments received."
                        }
                        systemMessagesToAppend.append(ChatMessage(text: toolMessageText, sender: .system, isToolResponse: true))
                        
                        if let callId = functionCall.id {
                            sendToolResponseMessage(id: callId, result: [:]) 
                        }
                    } else {
                        toolMessageText = "Received unhandled tool call: \(functionCall.name ?? "unknown")"
                        systemMessagesToAppend.append(ChatMessage(text: toolMessageText, sender: .system, isToolResponse: true))
                    }
                }
            }

            // 4. UsageMetadata ì²˜ë¦¬
            if let usage = wrapper.usageMetadata {
                var usageText = "Usage - Total Tokens: \(usage.totalTokenCount ?? 0)"
                if let promptTokens = usage.promptTokenCount, let responseTokens = usage.responseTokenCount {
                    usageText += " (Prompt: \(promptTokens), Response: \(responseTokens))"
                }
                systemMessagesToAppend.append(ChatMessage(text: "System: " + usageText, sender: .system))
            }

            // UI ì—…ë°ì´íŠ¸ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ)
            DispatchQueue.main.async {
                if let text = modelResponseText, !text.isEmpty {
                    self.chatMessages.append(ChatMessage(text: text, sender: .model))
                }
                self.chatMessages.append(contentsOf: systemMessagesToAppend)
            }

        } catch {
            print("âŒ Error decoding server message: \(error)")
        }
    }

    // MARK: - Tool Response Sender (NEW)
    func sendToolResponseMessage(id: String, result: [String: AnyCodableValue]) { // AnyCodableValueëŠ” ëª¨ë¸ íŒŒì¼ì— ì •ì˜ í•„ìš”
        guard isConnected else {
            print("Cannot send tool response: Not connected.")
            return
        }
        
        let functionResponse = FunctionResponse(id: id, response: result)
        let toolResponsePayload = ToolResponsePayload(functionResponses: [functionResponse])
        let messageToSend = ToolResponseMessage(toolResponse: toolResponsePayload)
        
        do {
            let encoder = JSONEncoder()
            encoder.keyEncodingStrategy = .convertToSnakeCase
            let jsonData = try encoder.encode(messageToSend)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
                print("Sent ToolResponseMessage: \(jsonString)")
            }
        } catch {
            print("Error encoding ToolResponseMessage: \(error)")
        }
    }

    // MARK: - Audio Handling Methods
    private func handleReceivedAudioData(base64String: String, mimeType: String) {
        // Stage 3ì—ì„œëŠ” ì£¼ê¸°ì  ê°€ì´ë˜ìŠ¤ë§Œ ì°¨ë‹¨í•˜ê³ , ì‚¬ìš©ì ì§ˆë¬¸ ì‘ë‹µì€ í—ˆìš©
        
        if !isAudioEngineSetup {
            setupAudioSession()
            setupAudioEngine()
            
            // ì¬ê²€ì¦
            guard isAudioEngineSetup else {
                print("âŒ GeminiClient: Failed to setup audio engine. Cannot play audio.")
                return
            }
        }
        
        // âœ… ì˜¤ë””ì˜¤ ì—”ì§„ ìƒíƒœ í™•ì¸
        if !audioEngine.isRunning {
            do {
                try audioEngine.start()
            } catch {
                return
            }
        }
        
        guard let audioData = Data(base64Encoded: base64String) else {
            print("âŒ GeminiClient: Could not decode base64 audio data.")
            return
        }
        
        // âœ… ë‹¨ìˆœí™”: ê³¼ë„í•œ ë¡œê¹… ì œê±°
        
        // ë””ì½”ë“œ ì™„ë£Œ
        
        // 1. PCM ë°ì´í„° í¬ë§· ì •ì˜
        guard let sourceFormat = audioOutputFormatForPCM else {
            print("âŒ GeminiClient: audioOutputFormatForPCM (sourceFormat) is nil.")
            return
        }
        
        // ì†ŒìŠ¤ í¬ë§· í™•ì¸

        // 2. PCM ë²„í¼ ìƒì„±
        let monoBytesPerFrame = Int(sourceFormat.streamDescription.pointee.mBytesPerFrame)
        if monoBytesPerFrame == 0 {
            print("Error: monoBytesPerFrame is zero.")
            return
        }
        let monoFrameCount = AVAudioFrameCount(audioData.count / monoBytesPerFrame)
        if monoFrameCount == 0 {
            print("Error: Calculated monoFrameCount is zero.")
            DispatchQueue.main.async {
                 self.chatMessages.append(ChatMessage(text: "System: Received audio data too small or invalid.", sender: .system))
            }
            return
        }
        
        guard let monoPCMBuffer = AVAudioPCMBuffer(pcmFormat: sourceFormat, frameCapacity: monoFrameCount) else {
            print("Error: Could not create monoPCMBuffer.")
            return
        }
        monoPCMBuffer.frameLength = monoFrameCount
        
        var dataCopiedSuccessfully = false
        audioData.withUnsafeBytes { (rawBufferPointer: UnsafeRawBufferPointer) in
            if let int16ChannelData = monoPCMBuffer.int16ChannelData, let sourceAddress = rawBufferPointer.baseAddress {
                let destinationPointer = UnsafeMutableRawBufferPointer(start: int16ChannelData[0], count: audioData.count)
                memcpy(destinationPointer.baseAddress!, sourceAddress, audioData.count)
                dataCopiedSuccessfully = true
            } else {
                print("Error: monoPCMBuffer.int16ChannelData is nil or rawBufferPointer.baseAddress is nil.")
            }
        }
        
        guard dataCopiedSuccessfully else {
            print("âŒ GeminiClient: Failed to copy audio data to monoPCMBuffer.")
            return
        }
        
        // ë²„í¼ë¡œ ë°ì´í„° ë³µì‚¬ ì™„ë£Œ

        // 3. íƒ€ê²Ÿ í¬ë§· ê°€ì ¸ì˜¤ê¸°
        let targetFormat = audioPlayerNode.outputFormat(forBus: 0)
        // íƒ€ê²Ÿ í¬ë§· í™•ì¸

        // 4. í¬ë§· ë³€í™˜ ë° ì¬ìƒ
        if sourceFormat.isEqual(targetFormat) {
            // âœ… ë‹¨ìˆœí™”: í¬ë§·ì´ ë™ì¼í•˜ë©´ ì¦‰ì‹œ ì¬ìƒ
            audioPlayerNode.scheduleBuffer(monoPCMBuffer)
        } else {
            // í¬ë§·ì´ ë‹¤ë¥´ë©´ ë³€í™˜ í•„ìš”
            // í¬ë§· ë¶ˆì¼ì¹˜ - ë³€í™˜ í•„ìš”
            guard let converter = AVAudioConverter(from: sourceFormat, to: targetFormat) else {
                print("âŒ GeminiClient: Could not create AVAudioConverter from \(sourceFormat) to \(targetFormat)")
                return
            }

            let outputFrameCapacity = AVAudioFrameCount(ceil(Double(monoPCMBuffer.frameLength) * (targetFormat.sampleRate / sourceFormat.sampleRate)))
            guard outputFrameCapacity > 0 else {
                print("Error: outputFrameCapacity is zero or negative (\(outputFrameCapacity)). Input frames: \(monoPCMBuffer.frameLength), SR Ratio: \(targetFormat.sampleRate / sourceFormat.sampleRate)")
                return
            }

            guard let convertedBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outputFrameCapacity) else {
                print("Error: Could not create convertedBuffer for targetFormat. Capacity: \(outputFrameCapacity)")
                return
            }

            var error: NSError?
            var inputBufferProvidedForThisConversion = false 

            // ì…ë ¥ ë¸”ë¡: ë³€í™˜ê¸°ì— ì›ë³¸ ë°ì´í„°ë¥¼ ì œê³µ
            let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
                if inputBufferProvidedForThisConversion {
                    outStatus.pointee = .endOfStream
                    return nil
                }
                outStatus.pointee = .haveData
                inputBufferProvidedForThisConversion = true
                return monoPCMBuffer
            }
            
            // ë³€í™˜ ì‹¤í–‰
            let status = converter.convert(to: convertedBuffer, error: &error, withInputFrom: inputBlock)

            if status == .error {
                print("Error during audio conversion: \(error?.localizedDescription ?? "Unknown error")")
                if let nsError = error {
                     DispatchQueue.main.async {
                         self.chatMessages.append(ChatMessage(text: "System: Audio conversion error - \(nsError.code)", sender: .system))
                    }
                }
                return
            }
            
            // ë³€í™˜ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì¬ìƒ
            if convertedBuffer.frameLength > 0 {
                // âœ… ë‹¨ìˆœí™”: ë²„í¼ í ì—†ì´ ì¦‰ì‹œ ì¬ìƒ
                audioPlayerNode.scheduleBuffer(convertedBuffer)
            }
        }
        
        // âœ… ë‹¨ìˆœí™”: í”Œë ˆì´ì–´ ì‹œì‘ (í•„ìš”í•œ ê²½ìš°)
        if !audioPlayerNode.isPlaying {
            audioPlayerNode.play()
        }
    }
    
    // MARK: - Audio Playback Control
    
    private func stopAudioPlayback() {
        guard isAudioEngineSetup else { return }
        
        // âœ… ë‹¨ìˆœí™”: ì˜¤ë””ì˜¤ ì¤‘ë‹¨ë§Œ í•˜ê³  ì—”ì§„ ì¬ì‹œì‘ ì œê±°
        audioPlayerNode.stop()
        audioPlayerNode.reset()
    }
    
    // MARK: - AI Speaking State Management
    // âœ… ë‹¨ìˆœí™”: ì„œë²„ ì‹ í˜¸ ê¸°ë°˜ìœ¼ë¡œë§Œ ìƒíƒœ ê´€ë¦¬
    
    private func handleAIResponseStart() {
        DispatchQueue.main.async {
            if !self.isAISpeaking {
                self.isAISpeaking = true
                self.lastAIResponseTime = Date()
            }
        }
    }
    
    private func handleAIResponseComplete(reason: String) {
        DispatchQueue.main.async {
            self.isAISpeaking = false
            self.hasPendingGuidanceRequest = false
            self.lastAIResponseTime = Date()
            
            // âœ… ë‹¨ìˆœí™”: ë²„í¼ í ì œê±°ë¡œ ì¶”ê°€ ì²˜ë¦¬ ë¶ˆí•„ìš”
        }
    }
    
    // checkIfAllAudioFinished ì œê±° - ë‹¨ìˆœí™”
    
    func canSendGuidanceRequest() -> Bool {
        return !isAISpeaking && !hasPendingGuidanceRequest
    }
    
    // MARK: - Audio Recording Methods
    
    // OLD version - to be replaced
    // func requestMicrophonePermission(completion: @escaping (Bool) -> Void) {
    //     AVAudioSession.sharedInstance().requestRecordPermission { granted in
    //         DispatchQueue.main.async {
    //             completion(granted)
    //         }
    //     }
    // }

    // NEW async version
    func requestMicrophonePermission() async -> Bool {
        await withCheckedContinuation { continuation in
            AVAudioSession.sharedInstance().requestRecordPermission { granted in
                continuation.resume(returning: granted)
            }
        }
    }
    
    // Made public so AppState can control audio recording during STT
    func startRecording() { 
        guard isConnected else {
            DispatchQueue.main.async {
                self.chatMessages.append(ChatMessage(text: "System: Cannot start recording - not connected", sender: .system))
            }
            return
        }
        
        guard isAudioEngineSetup else {
            return
        }
        
        
        // **ìˆ˜ì •: ì‹¤ì œ ë…¹ìŒ ì‹œì‘ ë¡œì§ ë³µì›**
        Task {
            // Microphone permission and start
            if await requestMicrophonePermission() { 
                await MainActor.run { 
                    self.startRecordingInternal()
                }
            } else {
                await MainActor.run {
                    self.chatMessages.append(ChatMessage(text: "System: Microphone permission denied for manual start.", sender: .system))
                }
                print("GeminiLiveAPIClient: Microphone permission denied during manual start.")
            }
        }
    }
    
    private func startRecordingInternal() {
        guard !isRecording else { 
            return 
        }
        
        // ì˜¤ë””ì˜¤ ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì´ˆê¸°í™”
        if !isAudioEngineSetup {
            setupAudioSession()
            setupAudioEngine()
        }
        
        audioQueue.async { [weak self] in
            guard let self = self else { return }
            
            // ì…ë ¥ íƒ­ ì„¤ì¹˜
            if self.installInputTap() {
                self.startRecordingTimer()
                DispatchQueue.main.async {
                    self.isRecording = true
                    self.chatMessages.append(ChatMessage(text: "System: Recording started", sender: .system))
                }
            } else {
                DispatchQueue.main.async {
                    self.chatMessages.append(ChatMessage(text: "System: Failed to start recording", sender: .system))
                }
            }
        }
    }
    
    func stopRecording() {
        guard isRecording else { 
            return 
        }
        
        audioQueue.async { [weak self] in
            guard let self = self else { return }
            
            // íƒ€ì´ë¨¸ ì •ì§€
            DispatchQueue.main.async {
                self.recordingTimer?.invalidate()
                self.recordingTimer = nil
            }
            
            // ì…ë ¥ íƒ­ ì œê±°
            if self.inputTapInstalled {
                self.audioEngine.inputNode.removeTap(onBus: 0)
                self.inputTapInstalled = false
            }
            
            // ë§ˆì§€ë§‰ ëˆ„ì ëœ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì†¡
            if !self.accumulatedAudioData.isEmpty {
                self.sendAccumulatedAudioData()
            }
            
            DispatchQueue.main.async {
                self.isRecording = false
                self.chatMessages.append(ChatMessage(text: "System: Recording stopped", sender: .system))
            }
        }
    }
    
    private func installInputTap() -> Bool {
        guard !inputTapInstalled else { return true }
        
        let inputNode = audioEngine.inputNode
        
        // ì˜¤ë””ì˜¤ ì—”ì§„ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
        if !audioEngine.isRunning {
            do {
                try audioEngine.start()
            } catch {
                return false
            }
        }
        
        let inputFormat = inputNode.outputFormat(forBus: 0)
        
        // Reduced logging - removed verbose format details
        
        do {
            // ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ íƒ­ ì„¤ì¹˜
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputFormat) { [weak self] buffer, time in
                self?.processAudioBuffer(buffer)
            }
            
            inputTapInstalled = true
            return true
        } catch {
            print("âŒ GeminiClient: Failed to install input tap: \(error)")
            return false
        }
    }
    
    private func processAudioBuffer(_ buffer: AVAudioPCMBuffer) {
        guard let targetFormat = audioInputFormatForEngine else { return }
        
        // í¬ë§· ë³€í™˜ì´ í•„ìš”í•œì§€ í™•ì¸
        let sourceFormat = buffer.format
        
        if sourceFormat.isEqual(targetFormat) {
            // í¬ë§·ì´ ë™ì¼í•˜ë©´ ì§ì ‘ ì‚¬ìš©
            saveAudioDataFromBuffer(buffer)
        } else {
            // í¬ë§· ë³€í™˜ í•„ìš”
            convertAndSaveAudioBuffer(buffer, to: targetFormat)
        }
    }
    
    private func convertAndSaveAudioBuffer(_ sourceBuffer: AVAudioPCMBuffer, to targetFormat: AVAudioFormat) {
        guard let converter = AVAudioConverter(from: sourceBuffer.format, to: targetFormat) else {
            print("Error: Could not create audio converter for recording")
            return
        }
        
        // ë³€í™˜ëœ ë²„í¼ í¬ê¸° ê³„ì‚°
        let outputFrameCapacity = AVAudioFrameCount(ceil(Double(sourceBuffer.frameLength) * 
                                                        (targetFormat.sampleRate / sourceBuffer.format.sampleRate)))
        
        guard let convertedBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: outputFrameCapacity) else {
            print("Error: Could not create converted buffer for recording")
            return
        }
        
        var error: NSError?
        var inputBufferProvided = false
        
        let inputBlock: AVAudioConverterInputBlock = { inNumPackets, outStatus in
            if inputBufferProvided {
                outStatus.pointee = .endOfStream
                return nil
            }
            outStatus.pointee = .haveData
            inputBufferProvided = true
            return sourceBuffer
        }
        
        let status = converter.convert(to: convertedBuffer, error: &error, withInputFrom: inputBlock)
        
        if status == .error {
            print("Error during audio conversion for recording: \(error?.localizedDescription ?? "Unknown error")")
            return
        }
        
        if convertedBuffer.frameLength > 0 {
            saveAudioDataFromBuffer(convertedBuffer)
        }
    }
    
    private func saveAudioDataFromBuffer(_ buffer: AVAudioPCMBuffer) {
        let frameLength = Int(buffer.frameLength)
        let channelCount = Int(buffer.format.channelCount)
        
        guard frameLength > 0 else {
            return
        }
        
        var audioData: Data?
        
        if buffer.format.commonFormat == .pcmFormatInt16 {
            guard let int16ChannelData = buffer.int16ChannelData else { 
                return 
            }
            let dataSize = frameLength * channelCount * MemoryLayout<Int16>.size
            audioData = Data(bytes: int16ChannelData[0], count: dataSize)
            
        } else if buffer.format.commonFormat == .pcmFormatFloat32 {
            guard let floatChannelData = buffer.floatChannelData else { 
                return 
            }
            
            // Float32ë¥¼ Int16ìœ¼ë¡œ ë³€í™˜
            var int16Array = Array<Int16>(repeating: 0, count: frameLength * channelCount)
            for frame in 0..<frameLength {
                for channel in 0..<channelCount {
                    let floatValue = floatChannelData[channel][frame]
                    let clampedValue = max(-1.0, min(1.0, floatValue))
                    int16Array[frame * channelCount + channel] = Int16(clampedValue * 32767.0)
                }
            }
            
            audioData = Data(bytes: int16Array, count: int16Array.count * MemoryLayout<Int16>.size)
            
        } else {
            return
        }
        
        guard let validAudioData = audioData else {
            return
        }
        
        // ëˆ„ì  ë°ì´í„°ì— ì¶”ê°€
        accumulatedAudioData.append(validAudioData)
    }
    
    private func startRecordingTimer() {
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            self.recordingTimer = Timer.scheduledTimer(withTimeInterval: self.recordingChunkDuration, repeats: true) { _ in
                self.audioQueue.async {
                    Task { @MainActor in
                        self.sendAccumulatedAudioData()
                    }
                }
            }
        }
    }
    
    private func sendAccumulatedAudioData() {
        guard !accumulatedAudioData.isEmpty else { 
            return 
        }
        
        let dataSize = accumulatedAudioData.count
        
        // Base64ë¡œ ì¸ì½”ë”©
        let base64Data = accumulatedAudioData.base64EncodedString()
        
        // ì‹¤ì‹œê°„ ì…ë ¥ ë©”ì‹œì§€ ìƒì„± ë° ì „ì†¡
        sendRealtimeAudioInput(base64Data: base64Data)
        
        // ë°ì´í„° ì´ˆê¸°í™”
        accumulatedAudioData = Data()
    }
    
    private func sendRealtimeAudioInput(base64Data: String) {
        // âœ… ë‹¨ìˆœí™”: ê³¼ë„í•œ ë¡œê¹… ì œê±°
        
        let mediaChunk = RealtimeMediaChunk(mimeType: "audio/pcm;rate=24000", data: base64Data)
        let realtimeInput = RealtimeInputPayload(mediaChunks: [mediaChunk])
        let message = RealtimeInputMessage(realtimeInput: realtimeInput)
        
        do {
            let encoder = JSONEncoder()
            let jsonData = try encoder.encode(message)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
            }
        } catch {
            print("âŒ GeminiLiveAPIClient: Audio encoding error: \(error)")
        }
    }

    // MARK: - URLSessionWebSocketDelegate

    nonisolated func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didOpenWithProtocol protocol: String?) {
        Task { @MainActor in
            self.isConnected = true
            self.lastConnectedTime = Date()
            self.chatMessages.append(ChatMessage(text: "System: WebSocket Connected!", sender: .system))
            
            // ì¬ì—°ê²° ì„±ê³µ ì‹œ ì¹´ìš´í„° ë¦¬ì…‹
            if self.reconnectAttempts > 0 {
                self.chatMessages.append(ChatMessage(
                    text: "System: Reconnection successful",
                    sender: .system
                ))
            }
            
            // ì¬ì—°ê²° ê´€ë ¨ ìƒíƒœ ë¦¬ì…‹
            cancelReconnectTimer()
            shouldAutoReconnect = true
            
            // âœ… ì˜¤ë””ì˜¤ ì„¸ì…˜ê³¼ ì—”ì§„ ì´ˆê¸°í™” (ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´)
            if !self.isAudioEngineSetup {
                self.setupAudioSession()
                self.setupAudioEngine()
            }
            
            // Send setup message with stored parameters
            if let params = setupParameters {
                sendSetupMessage(
                    modelName: params.modelName,
                    systemPrompt: params.systemPrompt,
                    voiceName: params.voiceName,
                    languageCode: params.languageCode,
                    includeGoogleSearch: params.includeGoogleSearch
                )
            }
            
            // âœ… ë‹¨ìˆœí™”: Stage 3 ëŒ€ê¸° ì¤‘ì´ë¼ë©´ ì¦‰ì‹œ í™œì„±í™”
            if pendingStage3Activation {
                pendingStage3Activation = false
                enablePureConversationMode()
            }
            
            // Start audio recording
            if await requestMicrophonePermission() {
                self.startRecordingInternal()
            } else {
                self.chatMessages.append(ChatMessage(text: "System: Microphone permission denied for auto-start.", sender: .system))
                print("Microphone permission denied during auto-start for GeminiLiveAPIClient.")
            }
            
            // Start receiving messages
            self.receiveMessagesLoop()
        }
    }

    nonisolated func urlSession(_ session: URLSession, webSocketTask: URLSessionWebSocketTask, didCloseWith closeCode: URLSessionWebSocketTask.CloseCode, reason: Data?) {
        Task { @MainActor in
            if self.isConnected {
                var reasonString = "Unknown reason"
                if let reasonData = reason, let str = String(data: reasonData, encoding: .utf8), !str.isEmpty {
                    reasonString = str
                }
                self.chatMessages.append(ChatMessage(text: "System: WebSocket Disconnected. Code: \(closeCode.rawValue), Reason: \(reasonString)", sender: .system))
            }
            self.isConnected = false
            
            var reasonStringLog = ""
            if let reason = reason, let str = String(data: reason, encoding: .utf8) {
                reasonStringLog = str
            }
            print("WebSocket connection closed: code \(closeCode.rawValue), reason: \(reasonStringLog)")
            self.webSocketTask = nil
            
            // ìë™ ì¬ì—°ê²° ì²˜ë¦¬
            handleConnectionClosed(closeCode: closeCode)
        }
    }
    
    // MARK: - Cleanup
    deinit {
        shouldAutoReconnect = false
        
        // MainActor ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹¤í–‰
        Task { @MainActor in
            cancelReconnectTimer()
            // ì˜¤ë””ì˜¤ ì„¸ì…˜ í•´ì œ
            AudioSessionCoordinator.shared.releaseAudioSession(for: .geminiLiveAudio)
        }
        
        recordingTimer?.invalidate()
        recordingTimer = nil
        
        // âœ… ë‹¨ìˆœí™”: íƒ€ì´ë¨¸ ê´€ë ¨ ì½”ë“œ ì œê±°ë¨
        
        if inputTapInstalled {
            audioEngine.inputNode.removeTap(onBus: 0)
            inputTapInstalled = false
        }
        
        if audioEngine.isRunning {
            audioEngine.stop()
        }
        
        webSocketTask?.cancel(with: .goingAway, reason: nil)
        webSocketTask = nil
    }

    // MARK: - Video Frame Processing
    
    // âœ… ìƒˆë¡œìš´ ì¦‰ì‹œ ë™ê¸°ì  í”„ë ˆì„ ì „ì†¡ ë©”ì„œë“œ (ë”œë ˆì´ ìµœì†Œí™”)
    func sendVideoFrameImmediately(pixelBuffer: CVPixelBuffer) {
        guard isConnected else {
            return
        }
        
        // âœ… ë¹„ë””ì˜¤ í™œì„±í™” (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
        if !isVideoEnabled {
            isVideoEnabled = true
        }
        
        // âœ… CVPixelBufferë¥¼ CIImageë¡œ ë³€í™˜ (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
        var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        
        // âœ… iOS ì¹´ë©”ë¼ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê°€ë¡œ ë°©í–¥ì´ë¯€ë¡œ ì„¸ë¡œë¡œ íšŒì „
        ciImage = ciImage.oriented(.right)
        
        // âœ… 0.5ë°° ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ë°ì´í„° í¬ê¸° ì¤„ì„
        let targetScale: CGFloat = 0.5
        let scaleTransform = CGAffineTransform(scaleX: targetScale, y: targetScale)
        ciImage = ciImage.transformed(by: scaleTransform)
        
        // âœ… JPEG ë°ì´í„° ìƒì„± (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì„±ëŠ¥ í–¥ìƒ)
        guard let jpegData = reusableCIContext.jpegRepresentation(
            of: ciImage,
            colorSpace: ciImage.colorSpace ?? CGColorSpaceCreateDeviceRGB(),
            options: [kCGImageDestinationLossyCompressionQuality as CIImageRepresentationOption: 0.8]
        ) else {
            print("âŒ GeminiLiveAPIClient: Failed to create JPEG data")
            return
        }
        
        // âœ… Base64 ì¸ì½”ë”©
        let base64ImageData = jpegData.base64EncodedString()
        
        // âœ… ë””ë²„ê·¸ ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ (ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
        debugProcessedImage = UIImage(data: jpegData)
        
        // âœ… Geminiì— ì¦‰ì‹œ ì „ì†¡
        sendRealtimeVideoFrame(base64Data: base64ImageData)
        
        // ë¡œê¹… ì œê±° (ì„±ëŠ¥ í–¥ìƒ)
    }
    
    // âœ… ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ í”„ë ˆì„ ì „ì†¡ ë©”ì„œë“œ (ë¡œê¹… ê°„ì†Œí™”)
    private func sendRealtimeVideoFrame(base64Data: String) {
        let mediaChunk = RealtimeMediaChunk(mimeType: "image/jpeg", data: base64Data)
        let realtimeInput = RealtimeInputPayload(mediaChunks: [mediaChunk])
        let message = RealtimeInputMessage(realtimeInput: realtimeInput)
        
        do {
            let encoder = JSONEncoder()
            let jsonData = try encoder.encode(message)
            if let jsonString = String(data: jsonData, encoding: .utf8) {
                sendString(jsonString)
                // ë¡œê¹… ì œê±° (ì„±ëŠ¥ í–¥ìƒ)
            }
        } catch {
            print("âŒ GeminiLiveAPIClient: Video frame encoding error: \(error)")
        }
    }
    
    // âœ… ê¸°ì¡´ ë©”ì„œë“œëŠ” ë ˆê±°ì‹œìš©ìœ¼ë¡œ ìœ ì§€í•˜ë˜ ê°œì„ 
    func processAndSendVideoFrame(pixelBuffer: CVPixelBuffer, orientation: CGImagePropertyOrientation = .up, timestamp: TimeInterval) {
        // âœ… ìƒˆë¡œìš´ ì¦‰ì‹œ ì „ì†¡ ë©”ì„œë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        sendVideoFrameImmediately(pixelBuffer: pixelBuffer)
    }

    // MARK: - Object Matching via Gemini API
    
    func findSimilarObject(koreanObjectName: String, availableObjects: [String], completion: @escaping (String?) -> Void) {
        
        // Clear English prompt for better matching
        let prompt = """
        You are helping with object detection matching. 
        
        User requested object in Korean: "\(koreanObjectName)"
        Available detected objects in English: \(availableObjects.joined(separator: ", "))
        
        Find the most similar English object name from the available list that matches the Korean object name.
        Reply with ONLY the exact English object name from the list, or "NOT_FOUND" if no reasonable match exists.
        
        Examples:
        - Korean "ì˜ì" should match English "chair"
        - Korean "ì±…ìƒ" should match English "table" or "dining table"  
        - Korean "ì¹¨ëŒ€" should match English "bed"
        - Korean "ì†ŒíŒŒ" should match English "couch"
        - Korean "ì»´í“¨í„°" should match English "laptop"
        - Korean "ë…¸íŠ¸ë¶" should match English "laptop"
        - Korean "í•¸ë“œí°" should match English "cell phone"
        
        Reply with only the object name or NOT_FOUND.
        """
        
        // Use REST API for quick object matching
        sendRESTRequest(prompt: prompt) { [weak self] response in
            DispatchQueue.main.async {
                // Trim and process
                let trimmed = response?.trimmingCharacters(in: .whitespacesAndNewlines)
                
                // Simply check if the result is in available objects
                let matchedObject: String?
                if let result = trimmed, result != "NOT_FOUND", !result.isEmpty {
                    // Convert both to lowercase for comparison
                    let resultLower = result.lowercased()
                    
                    // Try exact match first
                    matchedObject = availableObjects.first { availableObject in
                        return availableObject.lowercased() == resultLower
                    }
                    
                } else {
                    matchedObject = nil
                    // Result is nil, empty, or NOT_FOUND
                }
                
                
                completion(matchedObject)
            }
        }
    }
    
    private func sendRESTRequest(prompt: String, completion: @escaping (String?) -> Void) {
        // Simple REST API call to Gemini for object matching
        guard let url = URL(string: "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=\(apiKey)") else {
            completion(nil)
            return
        }
        
        // Send REST request
        
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.setValue("application/json", forHTTPHeaderField: "Content-Type")
        
        let requestBody: [String: Any] = [
            "contents": [
                [
                    "parts": [
                        ["text": prompt]
                    ]
                ]
            ]
        ]
        
        do {
            request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
            // Request prepared
        } catch {
            completion(nil)
            return
        }
        
        let startTime = Date()
        URLSession.shared.dataTask(with: request) { data, response, error in
            let duration = Date().timeIntervalSince(startTime)
            
            // Check HTTP response
            
            if let error = error {
                completion(nil)
                return
            }
            
            guard let data = data else {
                completion(nil)
                return
            }
            
            // Process response
            
            do {
                if let json = try JSONSerialization.jsonObject(with: data) as? [String: Any] {
                    // Check for error response
                    if let error = json["error"] as? [String: Any] {
                        completion(nil)
                        return
                    }
                    
                    // Extract text from response
                    if let candidates = json["candidates"] as? [[String: Any]],
                       let firstCandidate = candidates.first,
                       let content = firstCandidate["content"] as? [String: Any],
                       let parts = content["parts"] as? [[String: Any]],
                       let firstPart = parts.first,
                       let text = firstPart["text"] as? String {
                        completion(text)
                    } else {
                        completion(nil)
                    }
                } else {
                    completion(nil)
                }
            } catch {
                completion(nil)
            }
        }.resume()
    }

    
    // âœ… ìƒˆë¡œìš´ ë©”ì„œë“œ: AI ìƒíƒœ í”Œë˜ê·¸ ë¦¬ì…‹
    func resetAISpeakingState() {
        DispatchQueue.main.async {
            self.isAISpeaking = false
            self.hasPendingGuidanceRequest = false
            self.lastAIResponseTime = Date()
        }
    }
    
    // âœ… Stage 3 ììœ  ëŒ€í™” ëª¨ë“œ í™œì„±í™”
    func enablePureConversationMode() {
        guard isConnected else {
            return
        }
        
        // Stage 3 ì‹œì‘ í”„ë¡¬í”„íŠ¸ ì „ì†¡
        let prompt = """
        ë‹¹ì‹ ì€ ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ AI ê°€ì´ë“œì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìê°€ ëª©ì ì§€ ê·¼ì²˜ì— ë„ì°©í–ˆìŠµë‹ˆë‹¤.
        
        ì§€ê¸ˆ ì¦‰ì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì¸ì‚¬í•˜ê³  ë„ì›€ì„ ì œì•ˆí•˜ì„¸ìš”:
        "ì•ˆë…•í•˜ì„¸ìš”! ëª©ì ì§€ ê·¼ì²˜ì— ë„ì°©í•˜ì…¨ë„¤ìš”. ì£¼ë³€ í™˜ê²½ì´ë‚˜ ë¬¼ê±´ ìœ„ì¹˜ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´ ì£¼ì„¸ìš”. ì œê°€ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        
        ì´í›„ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        """
        sendUserText(prompt)
        
        // ë…¹ìŒì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if !isRecording {
            startRecording()
        }
    }

    // **ì¶”ê°€: GeminiClientìš© ìµœì‹  í”„ë ˆì„ ì œê³µ ë©”ì„œë“œ**
    func getCurrentVideoFrameForGemini() -> String? {
        // âœ… ARViewModelì—ì„œ í”„ë ˆì„ì„ ê°€ì ¸ì™€ì•¼ í•¨ (URLSessionì´ ì•„ë‹Œ ARSession í•„ìš”)
        guard let arViewModel = arViewModel else {
            print("âŒ GeminiLiveAPIClient: ARViewModel not available")
            return nil
        }
        
        // âœ… ARSessionì˜ currentFrame ì‚¬ìš©
        guard let currentFrame = arViewModel.session.currentFrame else {
            print("âŒ GeminiLiveAPIClient: No current frame from ARSession")
            return nil
        }
        
        // âœ… ì¦‰ì‹œ í•„ìš”í•œ ë°ì´í„°ë§Œ ë³µì‚¬í•˜ê³  ARFrame ì°¸ì¡° í•´ì œ
        let pixelBuffer = currentFrame.capturedImage
        
        // âœ… autoreleasepoolë¡œ ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ + ìì‹ ì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext ì‚¬ìš©
        return autoreleasepool {
            var ciImage = CIImage(cvPixelBuffer: pixelBuffer)
            ciImage = ciImage.oriented(.right)
            
            let targetScale: CGFloat = 0.5
            let scaleTransform = CGAffineTransform(scaleX: targetScale, y: targetScale)
            ciImage = ciImage.transformed(by: scaleTransform)
            
            // âœ… ìì‹ ì˜ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ CIContext ì‚¬ìš© (self ì‚¬ìš©)
            guard let jpegData = self.reusableCIContext.jpegRepresentation(
                of: ciImage,
                colorSpace: ciImage.colorSpace ?? CGColorSpaceCreateDeviceRGB(),
                options: [kCGImageDestinationLossyCompressionQuality as CIImageRepresentationOption: 0.8]
            ) else {
                print("âŒ GeminiLiveAPIClient: Failed to create JPEG from current frame")
                return nil
            }
            
            return jpegData.base64EncodedString()
        }
    }
    
    // MARK: - Network Resilience Methods
    
    private func handleConnectionClosed(closeCode: URLSessionWebSocketTask.CloseCode) {
        // ì •ìƒ ì¢…ë£Œë‚˜ ì‚¬ìš©ìê°€ ì˜ë„í•œ ì¢…ë£Œì¸ ê²½ìš° ì¬ì—°ê²°í•˜ì§€ ì•ŠìŒ
        guard shouldAutoReconnect,
              closeCode != .normalClosure,
              closeCode != .goingAway else {
            print("âœ… GeminiClient: Normal disconnection, not attempting reconnect")
            return
        }
        
        // ì¬ì—°ê²° ì‹œë„
        attemptReconnect()
    }
    
    private func attemptReconnect() {
        guard reconnectAttempts < maxReconnectAttempts else {
            print("âŒ GeminiClient: Max reconnection attempts reached")
            DispatchQueue.main.async {
                self.chatMessages.append(ChatMessage(
                    text: "System: Failed to reconnect after \(self.maxReconnectAttempts) attempts",
                    sender: .system
                ))
            }
            return
        }
        
        reconnectAttempts += 1
        
        print("ğŸ”„ GeminiClient: Attempting reconnection \(reconnectAttempts)/\(maxReconnectAttempts) in \(reconnectDelay)s")
        
        DispatchQueue.main.async {
            self.chatMessages.append(ChatMessage(
                text: "System: Reconnecting... (attempt \(self.reconnectAttempts)/\(self.maxReconnectAttempts))",
                sender: .system
            ))
        }
        
        // Exponential backoffìœ¼ë¡œ ì¬ì—°ê²° ìŠ¤ì¼€ì¤„
        reconnectTimer = Timer.scheduledTimer(withTimeInterval: reconnectDelay, repeats: false) { [weak self] _ in
            guard let self = self else { return }
            
            if let params = self.setupParameters {
                self.connect(
                    modelName: params.modelName,
                    systemPrompt: params.systemPrompt,
                    voiceName: params.voiceName,
                    languageCode: params.languageCode,
                    includeGoogleSearch: params.includeGoogleSearch
                )
            } else {
                self.connect() // ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œ ì—°ê²°
            }
        }
        
        // ë‹¤ìŒ ì‹œë„ë¥¼ ìœ„í•´ ë”œë ˆì´ ì¦ê°€ (exponential backoff)
        reconnectDelay = min(reconnectDelay * 2, maxReconnectDelay)
    }
    
    private func cancelReconnectTimer() {
        reconnectTimer?.invalidate()
        reconnectTimer = nil
        reconnectAttempts = 0
        reconnectDelay = 1.0
    }
    
    func resetConnection() {
        // ì—°ê²° ìƒíƒœ ì´ˆê¸°í™” ë° ì¬ì—°ê²°
        disconnect()
        shouldAutoReconnect = true
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) { [weak self] in
            guard let self = self else { return }
            if let params = self.setupParameters {
                self.connect(
                    modelName: params.modelName,
                    systemPrompt: params.systemPrompt,
                    voiceName: params.voiceName,
                    languageCode: params.languageCode,
                    includeGoogleSearch: params.includeGoogleSearch
                )
            }
        }
    }
    
    // ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§
    func checkConnectionHealth() -> Bool {
        guard isConnected,
              let lastConnected = lastConnectedTime else {
            return false
        }
        
        // 30ì´ˆ ì´ìƒ ì‘ë‹µì´ ì—†ìœ¼ë©´ ì—°ê²° ìƒíƒœ ì˜ì‹¬
        let timeSinceLastResponse = Date().timeIntervalSince(lastAIResponseTime)
        if timeSinceLastResponse > 30 {
            print("âš ï¸ GeminiClient: No response for \(Int(timeSinceLastResponse))s, connection may be stale")
            return false
        }
        
        return true
    }
} 
